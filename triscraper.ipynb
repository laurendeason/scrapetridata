{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CITY: 'New York' ; YEAR: 2012 \n",
      "CITY: 'New York' ; YEAR: 2013 \n",
      "CITY: 'New York' ; YEAR: 2014 \n",
      "CITY: 'New York' ; YEAR: 2015 \n",
      "CITY: 'Chicago' ; YEAR: 2004 \n",
      "CITY: 'Chicago' ; YEAR: 2005 \n",
      "CITY: 'Chicago' ; YEAR: 2008 \n",
      "CITY: 'Chicago' ; YEAR: 2009 \n",
      "CITY: 'Chicago' ; YEAR: 2010 \n",
      "CITY: 'Chicago' ; YEAR: 2011 \n",
      "CITY: 'Chicago' ; YEAR: 2012 \n",
      "CITY: 'Chicago' ; YEAR: 2013 \n",
      "CITY: 'Chicago' ; YEAR: 2014 \n",
      "CITY: 'Washington' ; YEAR: 2012 \n",
      "CITY: 'Washington' ; YEAR: 2013 \n",
      "CITY: 'Washington' ; YEAR: 2014 \n",
      "CITY: 'Washington' ; YEAR: 2015 \n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from selenium import webdriver\n",
    "import selenium.webdriver.support.ui as ui\n",
    "from ediblepickle import checkpoint\n",
    "import string\n",
    "import metadata as md\n",
    "#from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException, WebDriverException, StaleElementReferenceException\n",
    "import time\n",
    "\n",
    "resultsdir = \"./results/\"\n",
    "\n",
    "#define an output log file to store output from scraping\n",
    "logfile = open('./logs/triscraper.log', 'a')  #a appends here.  to overwrite previous log file, change to w\n",
    "\n",
    "#need to add in condition based on lastpagescountdown plus alternative conditions indicating we are on last page of results for given race\n",
    "#condition to indicate we are on last page of results for given year-race\n",
    "#\tif lastpagescountdown <= 0 or (tempdf['Place'][0] == 1 and page != 1):\n",
    "#\t   break\n",
    "\n",
    "def df_pickler(df,f):\n",
    "    df.to_csv(f, encoding='utf-8')    \n",
    "    \n",
    "def df_unpickler(f):\n",
    "    return pd.read_csv(f)  \n",
    "    \n",
    "    \n",
    "def findnextpage(driver, nextpglink):\n",
    "    #this takes in given driver and a dictionary mapping type of element to the element value that identifies the link to the next page\n",
    "    #can't use dictionary here instead of repeated if then else; tried already but it tries to find values in driver even when they aren't part of dictionary for given city\n",
    "    #print(\"In findnextpage!!\")                 \n",
    "    if nextpglink['css'] != 'missing':\n",
    "        #print (\"In if for css\")\n",
    "        return driver.find_element_by_css_selector(nextpglink['css'])\n",
    "    elif nextpglink['class'] != 'missing':\n",
    "        #print (\"In if for class\")\n",
    "        return driver.find_element_by_class_name(nextpglink['class'])\n",
    "    elif nextpglink['text'] != 'missing':\n",
    "        #print (\"In if for text\")\n",
    "        return driver.find_element_by_link_text(nextpglink['text'])\n",
    "    #if all fields in nextpglink are missing, it is as though we attempt to find a link and get a noelem    print(\"findnextpage returning NoSuchElementException\")suchentexception\n",
    "    else:\n",
    "        return NoSuchElementException\n",
    "    \n",
    "\n",
    "@checkpoint(key = string.Template('{1}.csv'), work_dir = resultsdir, pickler=df_pickler, unpickler=df_unpickler, refresh = False)\n",
    "def getresults(triathlon, savename, maxpages = 100):\n",
    "        #This function scrapes the results for the given year which are located at given url, returns as a dataframe\n",
    "        #url = string containing url with results\n",
    "        # columnlist = list of column headers in order that they appear on results page\n",
    "        #maxpages is max number of pages to loop through in case break condition fails\n",
    "\n",
    "        driver = webdriver.PhantomJS()\n",
    "        driver.set_window_size(1120, 550)\n",
    "\n",
    "        #this defines a maximium wait time of n seconds to load page after clicking next and checking condition - 10 worked before, now times out\n",
    "        wait = ui.WebDriverWait(driver,100)\n",
    "        #adds in an implicit wait time of 5 seconds in case where element isnt readily accesible (Default is 0) to try to avoid StaleElementReferenceException\n",
    "        #doesnt seem to work since element will still be there even if stale\n",
    "        #driver.implicitly_wait(5)\n",
    "        \n",
    "        startcountdown = 0 \n",
    "        lastpagescountdown = 2\n",
    "        \n",
    "        for page in range(1,maxpages):\n",
    "            if startcountdown == 1:\n",
    "                lastpagescountdown -= 1\n",
    "            logfile.write((\"Page is %d\" % page))\n",
    "            print(\"Page is %d\" % page)\n",
    "            if page == 1:\n",
    "                driver.get(triathlon.resulturl)\n",
    "            else:\n",
    "                try:\n",
    "                    #wait.until(lambda driver: findnextpage(driver, nextpglink).click() not in [WebDriverException, StaleElementReferenceException])\n",
    "                    findnextpage(driver, triathlon.nextpglink).click()\n",
    "                    #this should be modified to go back and try other elements identifying nextpglink before resorting to exception below, maybe just an if statement in findnextpage above\n",
    "                    #before first return , if nextpagmap[key].click() != NoSuchElementException\n",
    "                except NoSuchElementException:\n",
    "                    driver.find_element_by_link_text(str(page)).click();\n",
    "            \n",
    "            #first wait makes sure page has loaded such that i dont get StaleElementReferenceException looking for em.current\n",
    "            #need to figure out to make this work! see here for js solution: http://darrellgrainger.blogspot.it/2012/06/staleelementexception.html\n",
    "            #wait.until(not EC.staleness_of(driver.find_element_by_css_selector('em.current')))\n",
    "            time.sleep(5)\n",
    "            \n",
    "            #below if condition will always return true since EC doesnt return boolean\n",
    "            #if EC.staleness_of(driver.find_element_by_css_selector('em.current')):\n",
    "            #    print \"Stale!  waiting!\"\n",
    "            #    print EC.staleness_of(driver.find_element_by_css_selector('em.current'))\n",
    "            #    time.sleep(5)\n",
    "            #checks that new page has loaded by checking if highlighted page number is equal to page number in for loop\n",
    "            try:\n",
    "                wait.until(lambda driver: driver.find_element_by_css_selector(triathlon.currpagecss).text == unicode(page))\n",
    "            except TimeoutException:\n",
    "                logfile.write(\"WARNING: Current page, %r, doesn't match counted page, %r, allowing %r more pages to be read. \\n\" % (driver.find_element_by_css_selector(triathlon.currpagecss).text,page, lastpagescountdown+1))\n",
    "                startcountdown = 1\n",
    "            \n",
    "            soup = BeautifulSoup(driver.page_source, \"lxml\") #, \"html5lib\"\n",
    "            tridatalist = []\n",
    "            i=0\n",
    "            rowgen = (row for row in soup('table', triathlon.tableattributes)[0].tbody('tr') if len(row('td')) >  0)\n",
    "            for row in rowgen:\n",
    "                tridatalist.append([])\n",
    "                tds = row('td')\n",
    "                for j in range(len(tds)):\n",
    "                    if tds[j].stripped_strings is None:\n",
    "                        tridatalist[i].append(None)\n",
    "                    elif \"\".join(tds[j].stripped_strings) == '':\n",
    "                        tridatalist[i].append(None)\n",
    "                    elif triathlon.columns[j] in ['Place', 'Bib','Age','DIVplace', 'SEXplace', 'SWIMRANK', 'BIKERANK', 'RUNRANK', 'PENALTY']:\n",
    "                        tridatalist[i].append(int(\"\".join(tds[j].stripped_strings)))\n",
    "                    elif triathlon.columns[j] == 'MPH':\n",
    "                        tridatalist[i].append(float(\"\".join(tds[j].stripped_strings))) \n",
    "                    elif triathlon.columns[j] in ['swim', 'bike', 'run','FinishTime', 'T1', 'T2']:\n",
    "                        #for now just use explicit if statement for which city--should be able to merge conditions into one set of more universal\n",
    "                        if triathlon.racecode.split('_')[0] == 'CH':\n",
    "                            if \"\".join(tds[j].stripped_strings) == '0' or \"\".join(tds[j].stripped_strings)[2] != \":\" or \"\".join(tds[j].stripped_strings)[5] != \":\":\n",
    "                                tridatalist[i].append(None)\n",
    "                            else:\n",
    "                                t = datetime.datetime.strptime(\"\".join(tds[j].stripped_strings), \"%H:%M:%S\")\n",
    "                                tridatalist[i].append(datetime.timedelta(hours=t.hour, minutes=t.minute, seconds=t.second))\n",
    "                        elif triathlon.racecode.split('_')[0] == 'DC' or triname == 'NY':\n",
    "                            if \"\".join(tds[j].stripped_strings).count(':') == 0:\n",
    "                                tridatalist[i].append(None)\n",
    "                            elif \"\".join(tds[j].stripped_strings).count(':') == 1:    \n",
    "                                t = datetime.datetime.strptime(\"\".join(tds[j].stripped_strings), \"%M:%S\")\n",
    "                                tridatalist[i].append(datetime.timedelta(hours=t.hour, minutes=t.minute, seconds=t.second))\n",
    "                            elif \"\".join(tds[j].stripped_strings).count(':') == 2:    \n",
    "                                t = datetime.datetime.strptime(\"\".join(tds[j].stripped_strings), \"%H:%M:%S\")\n",
    "                                tridatalist[i].append(datetime.timedelta(hours=t.hour, minutes=t.minute, seconds=t.second))\n",
    "                    else:\n",
    "                        #tridatalist[i].append(unicode(tds[j].string))\n",
    "                        tridatalist[i].append(unicode(\"\".join(tds[j].stripped_strings))) \n",
    "                        \n",
    "                i += 1\n",
    "    \t\t\t\n",
    "    \t\n",
    "            tempdf = pd.DataFrame(tridatalist, columns = triathlon.columns)\n",
    "            tempdf['FinishTimeinHours'] = pd.TimedeltaIndex(tempdf['FinishTime']).days*24 + pd.TimedeltaIndex(tempdf['FinishTime']).seconds.astype(float)/3600 + pd.TimedeltaIndex(tempdf['FinishTime']).microseconds.astype(float)/3600000000\n",
    "            tempdf['bikeinHours'] = pd.TimedeltaIndex(tempdf['bike']).days*24 + pd.TimedeltaIndex(tempdf['bike']).seconds.astype(float)/3600 + pd.TimedeltaIndex(tempdf['bike']).microseconds.astype(float)/3600000000\n",
    "            tempdf['runinHours'] = pd.TimedeltaIndex(tempdf['run']).days*24 + pd.TimedeltaIndex(tempdf['run']).seconds.astype(float)/3600 + pd.TimedeltaIndex(tempdf['run']).microseconds.astype(float)/3600000000\n",
    "            \n",
    "            if 'T1' in triathlon.columns:\n",
    "                tempdf['T1inHours'] = pd.TimedeltaIndex(tempdf['T1']).days*24 + pd.TimedeltaIndex(tempdf['T1']).seconds.astype(float)/3600 + pd.TimedeltaIndex(tempdf['T1']).microseconds.astype(float)/3600000000\n",
    "            if 'T2' in triathlon.columns:            \n",
    "                tempdf['T2inHours'] = pd.TimedeltaIndex(tempdf['T2']).days*24 + pd.TimedeltaIndex(tempdf['T2']).seconds.astype(float)/3600 + pd.TimedeltaIndex(tempdf['T2']).microseconds.astype(float)/3600000000\t\n",
    "\n",
    "            \n",
    "            #assume that if swim is misisng, it was cancelled (such as DC 2014)\n",
    "            if 'swim' in triathlon.columns:\n",
    "                tempdf['swiminHours'] = pd.TimedeltaIndex(tempdf['swim']).days*24 + pd.TimedeltaIndex(tempdf['swim']).seconds.astype(float)/3600 + pd.TimedeltaIndex(tempdf['swim']).microseconds.astype(float)/3600000000\n",
    "            #else: \n",
    "            #    tempdf['swiminHours'] = tempdf['FinishTimeinHours']-tempdf['T1inHours']-tempdf['bikeinHours']-tempdf['T2inHours']-tempdf['runinHours']\n",
    "            #    tempdf['swim'] = pd.to_timedelta(tempdf['swiminHours'],unit='h')\n",
    "            \n",
    "            if page == 1:\n",
    "                df = tempdf\n",
    "            else:\n",
    "                df = pd.concat((df, tempdf), axis=0, ignore_index=True)\n",
    "            \n",
    "            logfile.write(tempdf.head().to_string())\n",
    "                \n",
    "            #condition to indicate we are on last page of results for given year-race -- currently this just adds together conditions from all 3 cities\n",
    "            if len(driver.find_elements_by_css_selector('span.next_page.disabled')) > 0 or lastpagescountdown <= 0 or (tempdf['Place'][0] == 1 and page != 1):\n",
    "                break\n",
    "            \n",
    "        #quit should close all phantomjs windows (could use close() to just close the one tab)\t\t\t\t\n",
    "        driver.quit()\t\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        return df\n",
    "                  \n",
    "@checkpoint(key = string.Template('{1}_allyears.csv'), work_dir = resultsdir, pickler=df_pickler, unpickler=df_unpickler, refresh = True)\n",
    "def getresultsallyears(trilist, savename):\n",
    "    #this accepts a triathlon object,2nd argument is simply the string under which you would like the results saved\n",
    "    #returns a dataframe storing the results data.\n",
    "\n",
    "    yearcount = 0\n",
    "\n",
    "    for tri in trilist:  \n",
    "        logfile.write(\"CITY: %r ; YEAR: %d \\n\" % (tri.city, tri.year) )\n",
    "        print(\"CITY: %r ; YEAR: %d \" % (tri.city, tri.year) )\n",
    "        tempdf = getresults(tri.year, tri.racecode) #second argument sent separately so that it can be used in naming csv file produced by ediblepickle\n",
    "        tempdf['Year'] = tri.year\n",
    "        tempdf['racecode'] = tri.racecode\n",
    "        tempdf['yearborn'] = (tempdf['Year']-tempdf['Age']).fillna(0.0).astype(int)  #assume bday has happened already in current year; later can match on yearborn or yearborn+1\n",
    "        tempdf[tempdf['yearborn'] == tempdf['Year']]['yearborn'] = None\n",
    "        if tri.nameformat == \"Firstname Lastname\":\n",
    "            tempdf['firstname'] = tempdf['Name'].str.rsplit(' ',expand=True,n=1)[0].str.strip().str.upper()\n",
    "            tempdf['lastname'] = tempdf['Name'].str.rsplit(' ',expand=True,n=1)[1].str.strip().str.upper()\n",
    "        elif tri.nameformat == \"Lastname, Firstname\":\n",
    "            tempdf['firstname'] = tempdf['Name'].str.split(',',expand=True,n=1)[1].str.strip().str.upper()\n",
    "            tempdf['lastname'] = tempdf['Name'].str.split(',',expand=True,n=1)[0].str.strip().str.upper() \n",
    "        \n",
    "        if yearcount == 0:\n",
    "            df = tempdf\n",
    "        else:\n",
    "            df = pd.concat((df, tempdf), axis=0, ignore_index=True)\n",
    "            \n",
    "        yearcount += 1\n",
    "        \n",
    "    #print('Returning df now')\n",
    "    return df   \n",
    "    \n",
    "@checkpoint(key = string.Template('allresults.csv'), work_dir = resultsdir, pickler=df_pickler, unpickler=df_unpickler, refresh = True)\n",
    "def aggregateresults(trilistdict):  \n",
    "    i = 0\n",
    "    for tri in trilistdict.keys():  #.keys() here isnt needed\n",
    "        if i == 0:\n",
    "            allresults = getresultsallyears(trilistdict[tri], tri)\n",
    "        else:\n",
    "            allresults = pd.concat((allresults, getresultsallyears(trilistdict[tri], tri)), axis=0, ignore_index=True)  \n",
    "        i+=1\n",
    "    return allresults\n",
    "   \n",
    "#trilistdict is now a dictionary of lists (of races)  \n",
    "allresults = aggregateresults(md.trilistdict)\n",
    "\n",
    "logfile.close()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
